{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4aba119e-8624-4c30-ad3c-49759f3f18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d70f3626-766c-430f-97d9-ee5c9068c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-04 07:58:09,351 WARNING: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "\n",
      "2025-03-04 12:58:09.358154\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "current_date = pd.to_datetime(datetime.utcnow())\n",
    "print(f\"{current_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98019378-2d7e-44d9-b6a3-794e640b2910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.tslibs.timestamps.Timestamp"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dfe29a0-f439-4e3b-8ad2-78eb40a3bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "current_date = pd.to_datetime(datetime.now(timezone.utc)).floor(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1ceac31-828d-4ca3-9ae3-19a0f4ec2ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64('2025-03-04T12:00:00.000000000')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_date.to_datetime64()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a9eb28f-e6ea-44e6-aa60-05d8304691fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data_to = current_date\n",
    "fetch_data_from = current_date - timedelta(days=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "133fcfda-af02-43f0-8851-63ef10b778ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-03-04 12:00:00+0000', tz='UTC')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_data_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec2f9e79-58b1-4e18-9346-c93fc04e1f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-02-03 12:00:00+0000', tz='UTC')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_data_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfaf545c-01d6-41dc-871d-c514c667bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "from src.data_utils import load_and_process_taxi_data\n",
    "\n",
    "def fetch_batch_raw_data(from_date: Union[datetime, str], to_date: Union[datetime, str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate production data by sampling historical data from 52 weeks ago (i.e., 1 year).\n",
    "\n",
    "    Args:\n",
    "        from_date (datetime or str): The start date for the data batch.\n",
    "        to_date (datetime or str): The end date for the data batch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the simulated production data.\n",
    "    \"\"\"\n",
    "    # Convert string inputs to datetime if necessary\n",
    "    if isinstance(from_date, str):\n",
    "        from_date = datetime.fromisoformat(from_date)\n",
    "    if isinstance(to_date, str):\n",
    "        to_date = datetime.fromisoformat(to_date)\n",
    "\n",
    "    # Validate input dates\n",
    "    if not isinstance(from_date, datetime) or not isinstance(to_date, datetime):\n",
    "        raise ValueError(\"Both 'from_date' and 'to_date' must be datetime objects or valid ISO format strings.\")\n",
    "    if from_date >= to_date:\n",
    "        raise ValueError(\"'from_date' must be earlier than 'to_date'.\")\n",
    "\n",
    "    # Shift dates back by 52 weeks (1 year)\n",
    "    historical_from_date = from_date - timedelta(weeks=52)\n",
    "    historical_to_date = to_date - timedelta(weeks=52)\n",
    "\n",
    "    # Load and filter data for the historical period\n",
    "    rides_from = load_and_process_taxi_data(year=historical_from_date.year, months=[historical_from_date.month])\n",
    "    rides_from = rides_from[rides_from.pickup_datetime >= historical_from_date]\n",
    "\n",
    "    if historical_to_date.month != historical_from_date.month:\n",
    "        rides_to = load_and_process_taxi_data(year=historical_to_date.year, months=[historical_to_date.month])\n",
    "        rides_to = rides_to[rides_to.pickup_datetime < historical_to_date]\n",
    "        # Combine the filtered data\n",
    "        rides = pd.concat([rides_from, rides_to], ignore_index=True)\n",
    "    else:\n",
    "        rides = rides_from\n",
    "    # Shift the data forward by 52 weeks to simulate recent data\n",
    "    rides['pickup_datetime'] += timedelta(weeks=52)\n",
    "\n",
    "    # Sort the data for consistency\n",
    "    rides.sort_values(by=['pickup_location_id', 'pickup_datetime'], inplace=True)\n",
    "\n",
    "    return rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e1669d5-cc0b-440b-8e8d-8bb78d16c58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists for 2024-02.\n",
      "Loading data for 2024-02...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 3,007,526\n",
      "Valid records: 2,954,709\n",
      "Records dropped: 52,817 (1.76%)\n",
      "Successfully processed data for 2024-02.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "File already exists for 2024-03.\n",
      "Loading data for 2024-03...\n",
      "Total records: 3,582,628\n",
      "Valid records: 3,518,066\n",
      "Records dropped: 64,562 (1.80%)\n",
      "Successfully processed data for 2024-03.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fetch_data_from = fetch_data_from.tz_convert(None)\n",
    "fetch_data_to = fetch_data_to.tz_convert(None)\n",
    "\n",
    "rides = fetch_batch_raw_data(fetch_data_from, fetch_data_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22176c04-e64c-496a-9acd-6a6eb7bc2ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>870353</th>\n",
       "      <td>2025-02-12 16:25:44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177054</th>\n",
       "      <td>2025-02-15 16:56:40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4644</th>\n",
       "      <td>2025-02-03 12:14:43</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39065</th>\n",
       "      <td>2025-02-03 18:07:44</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60769</th>\n",
       "      <td>2025-02-04 04:14:50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933617</th>\n",
       "      <td>2025-03-04 11:57:12</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932365</th>\n",
       "      <td>2025-03-04 11:57:23</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933802</th>\n",
       "      <td>2025-03-04 11:58:09</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932727</th>\n",
       "      <td>2025-03-04 11:58:41</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935202</th>\n",
       "      <td>2025-03-04 11:59:35</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2978940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickup_datetime  pickup_location_id\n",
       "870353  2025-02-12 16:25:44                   2\n",
       "1177054 2025-02-15 16:56:40                   2\n",
       "4644    2025-02-03 12:14:43                   3\n",
       "39065   2025-02-03 18:07:44                   3\n",
       "60769   2025-02-04 04:14:50                   3\n",
       "...                     ...                 ...\n",
       "2933617 2025-03-04 11:57:12                 263\n",
       "2932365 2025-03-04 11:57:23                 263\n",
       "2933802 2025-03-04 11:58:09                 263\n",
       "2932727 2025-03-04 11:58:41                 263\n",
       "2935202 2025-03-04 11:59:35                 263\n",
       "\n",
       "[2978940 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "783eebf6-7c7d-4ef8-9134-a92c87123146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import transform_raw_data_into_ts_data\n",
    "ts_data = transform_raw_data_into_ts_data(rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adbca6ba-f012-4f40-b419-ed620a8e7054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-03 12:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-03 13:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-03 14:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02-03 15:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-02-03 16:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174691</th>\n",
       "      <td>2025-03-04 07:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174692</th>\n",
       "      <td>2025-03-04 08:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174693</th>\n",
       "      <td>2025-03-04 09:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174694</th>\n",
       "      <td>2025-03-04 10:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174695</th>\n",
       "      <td>2025-03-04 11:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174696 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pickup_hour  pickup_location_id  rides\n",
       "0      2025-02-03 12:00:00                   2      0\n",
       "1      2025-02-03 13:00:00                   2      0\n",
       "2      2025-02-03 14:00:00                   2      0\n",
       "3      2025-02-03 15:00:00                   2      0\n",
       "4      2025-02-03 16:00:00                   2      0\n",
       "...                    ...                 ...    ...\n",
       "174691 2025-03-04 07:00:00                 263    130\n",
       "174692 2025-03-04 08:00:00                 263    152\n",
       "174693 2025-03-04 09:00:00                 263    134\n",
       "174694 2025-03-04 10:00:00                 263    116\n",
       "174695 2025-03-04 11:00:00                 263     98\n",
       "\n",
       "[174696 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b8f1dc6-46d6-4fad-915b-c8c4c02b9887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 174696 entries, 0 to 174695\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   pickup_hour         174696 non-null  datetime64[ns]\n",
      " 1   pickup_location_id  174696 non-null  int32         \n",
      " 2   rides               174696 non-null  int32         \n",
      "dtypes: datetime64[ns](1), int32(2)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c2d5acd-0f97-4117-b61d-6135391891db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-04 07:58:19,239 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-04 07:58:19,282 INFO: Initializing external client\n",
      "2025-03-04 07:58:19,283 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-04 07:58:20,065 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1214681\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "# connect to the project\n",
    "project = hopsworks.login(\n",
    "    project=config.HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value=config.HOPSWORKS_API_KEY\n",
    ")\n",
    "\n",
    "# connect to the feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# connect to the feature group\n",
    "feature_group = feature_store.get_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3093058-9e94-4b49-9ad9-32dace742a31",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureStoreException",
     "evalue": "Features are not compatible with Feature Group schema: \n - pickup_location_int (type: 'int') is missing from input dataframe.\n - pickup_location_id (type: 'int') does not exist in feature group.\nNote that feature (or column) names are case insensitive and spaces are automatically replaced with underscores.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwait_for_job\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kolpe\\anaconda3\\envs\\py312\\Lib\\site-packages\\hsfs\\feature_group.py:2959\u001b[0m, in \u001b[0;36mFeatureGroup.insert\u001b[1;34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[0m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2957\u001b[0m     write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline_backfill_every_hr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr\n\u001b[1;32m-> 2959\u001b[0m job, ge_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_group_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2960\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2963\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_type()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m   2972\u001b[0m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[0;32m   2973\u001b[0m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[0;32m   2974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_statistics()\n",
      "File \u001b[1;32mc:\\Users\\kolpe\\anaconda3\\envs\\py312\\Lib\\site-packages\\hsfs\\core\\feature_group_engine.py:188\u001b[0m, in \u001b[0;36mFeatureGroupEngine.insert\u001b[1;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_feature_group_metadata(\n\u001b[0;32m    184\u001b[0m         feature_group, dataframe_features, write_options\n\u001b[0;32m    185\u001b[0m     )\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# else, just verify that feature group schema matches user-provided dataframe\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_schema_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe_features\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# ge validation on python and non stream feature groups on spark\u001b[39;00m\n\u001b[0;32m    193\u001b[0m ge_report \u001b[38;5;241m=\u001b[39m feature_group\u001b[38;5;241m.\u001b[39m_great_expectation_engine\u001b[38;5;241m.\u001b[39mvalidate(\n\u001b[0;32m    194\u001b[0m     feature_group\u001b[38;5;241m=\u001b[39mfeature_group,\n\u001b[0;32m    195\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mfeature_dataframe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m     ge_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    199\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kolpe\\anaconda3\\envs\\py312\\Lib\\site-packages\\hsfs\\core\\feature_group_base_engine.py:171\u001b[0m, in \u001b[0;36mFeatureGroupBaseEngine._verify_schema_compatibility\u001b[1;34m(self, feature_group_features, dataframe_features)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# raise exception if any errors were found.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(err) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures are not compatible with Feature Group schema: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m err])\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote that feature (or column) names are case insensitive and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspaces are automatically replaced with underscores.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m     )\n",
      "\u001b[1;31mFeatureStoreException\u001b[0m: Features are not compatible with Feature Group schema: \n - pickup_location_int (type: 'int') is missing from input dataframe.\n - pickup_location_id (type: 'int') does not exist in feature group.\nNote that feature (or column) names are case insensitive and spaces are automatically replaced with underscores."
     ]
    }
   ],
   "source": [
    "feature_group.insert(ts_data, write_options={\"wait_for_job\": False})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
